import os
import time
import json
import requests
import urllib.parse
import numpy as np
import fitz  # PyMuPDF
from datetime import datetime
from typing import List, Dict, Optional
from dotenv import load_dotenv

# Selenium
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, ElementClickInterceptedException
from webdriver_manager.chrome import ChromeDriverManager

# LangChain & AI
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì „ì—­ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DATA_DIR = os.path.join(BASE_DIR, "data")
DOWNLOAD_DIR = os.path.join(DATA_DIR, "risk_data")
HISTORY_DIR = os.path.join(DATA_DIR, "crawling")
HISTORY_FILE = os.path.join(HISTORY_DIR, "risk_history.json")
VECTOR_DB_DIR = os.path.join(BASE_DIR, "vector_db", "esg_all")

# --------------------------------------------------------------------------
# [ì„¤ì •] ë¦¬ìŠ¤í¬ ì§„ë‹¨ ìë£Œ íƒ€ê²Ÿ ëª©ë¡
# --------------------------------------------------------------------------
RISK_TARGETS = [
    # 1. [ESG Hub] ESG ê¸ˆìœµ ì¶”ì§„ë‹¨ (E/S/G íƒœê·¸ ìˆ˜ì§‘)
    {
        "name": "ESG_Finance_Hub",
        "url": "https://www.esgfinancehub.or.kr/portal/report/imgList/vw/20211222092216000024",
        "type": "ESG_HUB", 
        "category": "ESG_General"
    },
    # 2. [Safety] ì•ˆì „ë³´ê±´ê³µë‹¨ ìë£Œë§ˆë‹¹
    {
        "name": "KOSHA_Construction_Guide",
        "url": "https://portal.kosha.or.kr/archive/resources/tech-support/search/const?page=1&rowsPerPage=10",
        "type": "KOSHA_ARCHIVE", 
        "category": "Safety"
    },
    # 3. [Safety] ê³ ìš©ë…¸ë™ë¶€ - ìœ„í—˜ì„±í‰ê°€ (Google ìš°íšŒ)
    {
        "name": "MOEL_Risk_Standard",
        "url": "https://www.moel.go.kr/info/publict/publictDataList.do", 
        "google_query": 'site:moel.go.kr filetype:pdf "ìœ„í—˜ì„±í‰ê°€" "í‘œì¤€ëª¨ë¸"',
        "type": "GOV_BOARD",
        "category": "Safety"
    },
    # 4. [Labor] ê³ ìš©ë…¸ë™ë¶€ - ììœ¨ì ê²€í‘œ (Google ìš°íšŒ)
    {
        "name": "MOEL_Checklist",
        "url": "https://www.moel.go.kr/news/notice/noticeList.do",
        "google_query": 'site:moel.go.kr filetype:pdf "ììœ¨ì ê²€í‘œ"',
        "type": "GOV_BOARD",
        "category": "Labor"
    },
    # 5. [Env] í™˜ê²½ë¶€ - ë¹„ì‚°ë¨¼ì§€ (Google ìš°íšŒ)
    {
        "name": "ME_Dust_Manual",
        "url": "https://www.me.go.kr/home/web/board/list.do?menuId=10392&boardMasterId=39",
        "google_query": 'site:me.go.kr filetype:pdf "ë¹„ì‚°ë¨¼ì§€" "ë§¤ë‰´ì–¼"',
        "type": "GOV_BOARD",
        "category": "Environment"
    },
    # 6. [Gov] ê³µì •ê±°ë˜ìœ„ì›íšŒ - í‘œì¤€ê³„ì•½ì„œ (Google ìš°íšŒ)
    {
        "name": "FTC_Construction_Contract",
        "url": "https://www.ftc.go.kr/www/cop/bbs/selectBoardList.do?key=201&bbsId=BBSMSTR_000000002320",
        "google_query": 'site:ftc.go.kr filetype:hwp OR filetype:pdf "ê±´ì„¤ì—…" "í‘œì¤€í•˜ë„ê¸‰ê³„ì•½ì„œ"',
        "type": "GOV_BOARD",
        "category": "Governance"
    }
]

class RiskCrawlingTool:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RiskCrawlingTool, cls).__new__(cls)
            cls._instance._initialize()
        return cls._instance

    def _initialize(self):
        print("âš™ï¸ [RiskTool] ì´ˆê¸°í™” ì¤‘...")
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-m3",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.embeddings = None

        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

        if self.embeddings:
            os.makedirs(VECTOR_DB_DIR, exist_ok=True)
            self.vector_db = Chroma(
                collection_name="esg_risk_guides",
                embedding_function=self.embeddings,
                persist_directory=VECTOR_DB_DIR
            )
        else:
            self.vector_db = None

        os.makedirs(DOWNLOAD_DIR, exist_ok=True)
        os.makedirs(HISTORY_DIR, exist_ok=True)
        self.history = self._load_history()

    def _load_history(self) -> Dict:
        if os.path.exists(HISTORY_FILE):
            try:
                with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except: return {}
        return {}

    def _save_history(self):
        try:
            with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, ensure_ascii=False, indent=2)
        except: pass

    def _is_processed(self, key: str) -> bool:
        return key in self.history

    def _mark_as_processed(self, key: str, title: str, files: List[str]):
        self.history[key] = {
            "title": title,
            "processed_at": datetime.now().isoformat(),
            "files": files
        }
        self._save_history()

    def _get_chrome_driver(self):
        chrome_options = Options()
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36")
        
        prefs = {
            "download.default_directory": DOWNLOAD_DIR,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True,
            "plugins.always_open_pdf_externally": True,
            "profile.default_content_settings.popups": 0,
            "profile.content_settings.exceptions.automatic_downloads.*.setting": 1
        }
        chrome_options.add_experimental_option("prefs", prefs)
        
        service = ChromeService(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        return driver

    def _extract_text_preview(self, pdf_path: str, max_pages: int = 5) -> str:
        text = ""
        try:
            doc = fitz.open(pdf_path)
            for i, page in enumerate(doc):
                if i >= max_pages: break
                text += page.get_text()
            doc.close()
        except: pass
        return text

    def _analyze_and_store(self, file_path: str, title: str, target_info: Dict) -> bool:
        if not self.vector_db or not file_path.lower().endswith('.pdf'):
            return False

        filename = os.path.basename(file_path)
        print(f"   ğŸ§  [AI ë¶„ì„] '{filename}' ì‹¤ë¬´ í™œìš©ë„ í‰ê°€ ì¤‘...")
        
        content_preview = self._extract_text_preview(file_path)
        if not content_preview: return False

        category_context = target_info['category']
        # ESG Hubì˜ ê²½ìš° ì´ë¯¸ ìˆ˜ì§‘ëœ sub_category(E/S/G)ë¥¼ í™œìš©
        if target_info.get("type") == "ESG_HUB":
            category_context = f"ESG_Specialized ({target_info.get('sub_category', 'General')})"

        prompt = f"""
        ë¬¸ì„œ ì œëª©: {title}
        ì¹´í…Œê³ ë¦¬: {category_context}
        ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:
        {content_preview[:2500]}

        ì´ ë¬¸ì„œê°€ ê¸°ì—… í˜„ì¥ì—ì„œ ì•ˆì „/í™˜ê²½/ë…¸ë¬´/ê±°ë²„ë„ŒìŠ¤ ë¦¬ìŠ¤í¬ë¥¼ ì ê²€í•˜ê±°ë‚˜ ESG ê²½ì˜ì— í™œìš©í•  ìˆ˜ ìˆëŠ” **ì‹¤ë¬´ ìë£Œ**ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.
        
        [íŒë‹¨ ê¸°ì¤€]
        - **ìœ ìš©í•¨ (True)**: ì²´í¬ë¦¬ìŠ¤íŠ¸, ê°€ì´ë“œë¼ì¸, ë§¤ë‰´ì–¼, í‘œì¤€ê³„ì•½ì„œ, ESG í‰ê°€ ì§€í‘œ í•´ì„¤.
        - **ìœ ìš©í•˜ì§€ ì•ŠìŒ (False)**: ë‹¨ìˆœ í–‰ì‚¬ ì•Œë¦¼, ë‰´ìŠ¤ë ˆí„°, ì¸ì‚¬ ë°œë ¹.

        ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì¶œë ¥:
        {{
            "is_practical": true/false,
            "doc_type": "Checklist/Manual/Contract/Guide",
            "score": (1~10),
            "esg_tag": "E/S/G/Common",
            "summary": "í•œ ì¤„ ìš”ì•½"
        }}
        """
        
        try:
            response = self.llm.invoke(prompt)
            result = json.loads(response.content.replace("```json", "").replace("```", "").strip())
            
            print(f"      ğŸ‘‰ ê²°ê³¼: {result['doc_type']} (ì ìˆ˜: {result['score']}, íƒœê·¸: {result.get('esg_tag')})")

            if result['is_practical'] and result['score'] >= 7:
                print(f"      ğŸ’¾ [Vector DB] ì €ì¥í•©ë‹ˆë‹¤.")
                
                full_doc = fitz.open(file_path)
                full_text = ""
                for page in full_doc:
                    full_text += page.get_text()
                full_doc.close()

                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                chunks = text_splitter.create_documents(
                    [full_text], 
                    metadatas=[{
                        "source": target_info['name'],
                        "category": target_info['category'],
                        "esg_tag": result.get('esg_tag', 'Common'),
                        "title": title,
                        "doc_type": result['doc_type'],
                        "filename": filename,
                        "crawled_at": datetime.now().isoformat()
                    }]
                )
                self.vector_db.add_documents(chunks)
                print(f"      âœ… DB ì €ì¥ ì™„ë£Œ ({len(chunks)} chunks)")
                return True
            else:
                print("      ğŸ—‘ï¸ [Skip] ì‹¤ë¬´ í™œìš©ë„ê°€ ë‚®ì•„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            print(f"      âŒ AI ë¶„ì„ ì˜¤ë¥˜: {e}")
            return False

    def _wait_for_download(self, before_files: set, title: str, target_info: Dict) -> bool:
        """ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° (ì‹œê°„ ì¦ê°€)"""
        # 30ì´ˆ ëŒ€ê¸°
        for i in range(30):
            time.sleep(1)
            current_files = set(os.listdir(DOWNLOAD_DIR))
            new_files = current_files - before_files
            
            if new_files:
                for new_file in new_files:
                    if not new_file.endswith('.crdownload') and not new_file.endswith('.tmp'):
                        full_path = os.path.join(DOWNLOAD_DIR, new_file)
                        if os.path.getsize(full_path) > 0:
                            print(f"      âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {new_file}")
                            self._analyze_and_store(full_path, title, target_info)
                            return True
        return False

    # ----------------------------------------------------------------
    # [Crawling] 3. ESG Finance Hub (ë©”ë‰´ í´ë¦­ + ì²´í¬ë°•ìŠ¤ + ê²€ìƒ‰ ë²„íŠ¼)
    # ----------------------------------------------------------------
    def _scrape_esg_finance_hub(self, driver, target_info: Dict) -> List[Dict]:
        """
        ESG ê¸ˆìœµ ì¶”ì§„ë‹¨ ë³´ê³ ì„œ í¬ë¡¤ëŸ¬ (ê°œì„  ë²„ì „)
        - ë©”ì¸ í˜ì´ì§€ì—ì„œ ë©”ë‰´ í´ë¦­ìœ¼ë¡œ ì ‘ê·¼
        - E/S/G ì²´í¬ë°•ìŠ¤ í´ë¦­
        - í•˜ìœ„ í•­ëª© ì„ íƒ
        - ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ (í•µì‹¬!)
        - button.file-btnìœ¼ë¡œ PDF ë‹¤ìš´ë¡œë“œ
        """
        name = target_info["name"]
        results = []
        
        print(f"ğŸ“¡ [{name}] ì ‘ì† ì¤‘...")
        try:
            # Step 1: ë©”ì¸ í˜ì´ì§€ ì ‘ì†
            main_url = "https://www.esgfinancehub.or.kr"
            driver.get(main_url)
            time.sleep(3)
            
            print("   ğŸ” ë©”ë‰´ íƒìƒ‰ ì¤‘...")
            
            # Step 2: "ê°€ì´ë“œë¼ì¸" > "ESGê³µì‹œ" ë©”ë‰´ í´ë¦­
            try:
                from selenium.webdriver.common.action_chains import ActionChains
                # ê°€ì´ë“œë¼ì¸ ë©”ë‰´ í˜¸ë²„
                guideline_menu = driver.find_element(By.XPATH, "//a[contains(text(), 'ê°€ì´ë“œë¼ì¸')]")
                actions = ActionChains(driver)
                actions.move_to_element(guideline_menu).perform()
                time.sleep(1)
                
                # ESGê³µì‹œ ì„œë¸Œë©”ë‰´ í´ë¦­
                esg_submenu = driver.find_element(By.XPATH, "//a[contains(text(), 'ESGê³µì‹œ')]")
                driver.execute_script("arguments[0].click();", esg_submenu)
                time.sleep(4)
                print("   âœ“ ESGê³µì‹œ í˜ì´ì§€ ì ‘ì† ì™„ë£Œ")
                
            except Exception as e:
                print(f"   âš ï¸ ë©”ë‰´ í´ë¦­ ì‹¤íŒ¨, ì§ì ‘ URL ì‹œë„: {e}")
                # ëŒ€ì²´: ì§ì ‘ URL
                driver.get(target_info["url"])
                time.sleep(4)
            
            # Step 3: ì²´í¬ë°•ìŠ¤ ë¡œë”© ëŒ€ê¸°
            wait = WebDriverWait(driver, 15)
            try:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "input[type='checkbox']")))
                print("   âœ“ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
                time.sleep(2)
            except TimeoutException:
                print("   âš ï¸ íƒ€ì„ì•„ì›ƒ")
            
            # Step 4: E, S, G ê° ì¹´í…Œê³ ë¦¬ ìˆœíšŒ
            esg_categories = [
                {'code': 'E', 'name': 'Environment'},
                {'code': 'S', 'name': 'Social'},
                {'code': 'G', 'name': 'Governance'}
            ]
            
            for esg_cat in esg_categories:
                try:
                    print(f"\n{'='*60}")
                    print(f"   ğŸ¯ [{esg_cat['code']}] ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹œì‘")
                    print(f"{'='*60}")
                    
                    # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                    driver.refresh()
                    time.sleep(4)
                    
                    # Step 5: ë©”ì¸ ì¹´í…Œê³ ë¦¬ ì²´í¬ë°•ìŠ¤ ì°¾ê¸°
                    category_checkbox = None
                    all_checkboxes = driver.find_elements(By.CSS_SELECTOR, "input[type='checkbox']")
                    
                    for cb in all_checkboxes:
                        try:
                            parent = cb.find_element(By.XPATH, "./..")
                            text = parent.text.strip()
                            
                            # "E (33)", "S (10)", "G (5)" íŒ¨í„´ ë§¤ì¹­
                            if text.startswith(f"{esg_cat['code']} ("):
                                category_checkbox = cb
                                print(f"      âœ“ ë°œê²¬: {text}")
                                break
                        except:
                            continue
                    
                    if not category_checkbox:
                        print(f"      âŒ {esg_cat['code']} ì²´í¬ë°•ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        continue
                    
                    # Step 6: ë©”ì¸ ì¹´í…Œê³ ë¦¬ í´ë¦­ (í¼ì¹˜ê¸°)
                    driver.execute_script("arguments[0].scrollIntoView(true);", category_checkbox)
                    time.sleep(1)
                    driver.execute_script("arguments[0].click();", category_checkbox)
                    time.sleep(2)
                    print(f"      âœ“ {esg_cat['code']} í¼ì¹¨")
                    
                    # Step 7: í•˜ìœ„ í•­ëª© ì°¾ê¸°
                    print(f"      ğŸ” í•˜ìœ„ í•­ëª© ê²€ìƒ‰ ì¤‘...")
                    time.sleep(2)
                    
                    all_checkboxes = driver.find_elements(By.CSS_SELECTOR, "input[type='checkbox']")
                    sub_items = []
                    
                    for cb in all_checkboxes:
                        try:
                            parent = cb.find_element(By.XPATH, "./..")
                            label = parent.text.strip()
                            
                            # ë©”ì¸ ì¹´í…Œê³ ë¦¬ ì œì™¸
                            if (label and 
                                not label.startswith('E (') and
                                not label.startswith('S (') and
                                not label.startswith('G (') and
                                2 < len(label) < 50):
                                
                                sub_items.append({
                                    'checkbox': cb,
                                    'label': label
                                })
                        except:
                            continue
                    
                    print(f"      ğŸ“‹ {len(sub_items)}ê°œ í•˜ìœ„ í•­ëª© ë°œê²¬")
                    
                    # Step 8: ê° í•˜ìœ„ í•­ëª© ì²˜ë¦¬ (ìµœëŒ€ 2ê°œë¡œ ì œí•œ - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
                    for idx, sub_item in enumerate(sub_items[:2]):
                        try:
                            sub_label = sub_item['label']
                            print(f"      [{idx+1}] {sub_label}")
                            
                            # í•˜ìœ„ ì²´í¬ë°•ìŠ¤ í´ë¦­
                            sub_checkbox = sub_item['checkbox']
                            driver.execute_script("arguments[0].scrollIntoView(true);", sub_checkbox)
                            time.sleep(0.5)
                            
                            if not sub_checkbox.is_selected():
                                driver.execute_script("arguments[0].click();", sub_checkbox)
                                time.sleep(1)
                            
                            # Step 9: **ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­** (í•µì‹¬!)
                            print(f"         ğŸ” ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì¤‘...")
                            try:
                                search_button = driver.find_element(By.XPATH, "//button[contains(text(), 'ê²€ìƒ‰')]")
                                driver.execute_script("arguments[0].scrollIntoView(true);", search_button)
                                time.sleep(0.5)
                                driver.execute_script("arguments[0].click();", search_button)
                                time.sleep(3)
                                print(f"         âœ“ ê²€ìƒ‰ ì™„ë£Œ")
                            except Exception as search_err:
                                print(f"         âš ï¸ ê²€ìƒ‰ ë²„íŠ¼ ì˜¤ë¥˜: {search_err}")
                            
                            # Step 10: PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì°¾ê¸°
                            print(f"         ğŸ“„ PDF íŒŒì¼ ì°¾ê¸° ì¤‘...")
                            
                            # button.file-btn ì°¾ê¸°
                            download_buttons = driver.find_elements(By.CSS_SELECTOR, "button.file-btn")
                            
                            if not download_buttons:
                                # onclickì— fileDown í¬í•¨ëœ ë²„íŠ¼ ì°¾ê¸°
                                all_buttons = driver.find_elements(By.TAG_NAME, "button")
                                download_buttons = [btn for btn in all_buttons 
                                                  if 'fileDown' in (btn.get_attribute('onclick') or '')]
                            
                            print(f"         ğŸ“¥ {len(download_buttons)}ê°œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ë°œê²¬")
                            
                            # ìµœëŒ€ 1ê°œë§Œ ë‹¤ìš´ë¡œë“œ (ë¹ ë¥¸ ì²˜ë¦¬)
                            for btn_idx, dl_button in enumerate(download_buttons[:1]):
                                try:
                                    file_name = dl_button.text.strip() or f"{sub_label}_{btn_idx+1}.pdf"
                                    
                                    unique_key = f"{name}_{esg_cat['code']}_{sub_label}_{file_name}"
                                    
                                    if self._is_processed(unique_key):
                                        print(f"         â­ï¸ [Skip] {file_name[:50]}")
                                        continue
                                    
                                    print(f"         ğŸ“¥ [{btn_idx+1}] {file_name[:50]}")
                                    
                                    before_files = set(os.listdir(DOWNLOAD_DIR))
                                    driver.execute_script("arguments[0].click();", dl_button)
                                    time.sleep(2)
                                    
                                    # ë‹¤ìš´ë¡œë“œ ëŒ€ê¸°
                                    target_info_with_sub = target_info.copy()
                                    target_info_with_sub['sub_category'] = esg_cat['name']
                                    
                                    downloaded_files = []
                                    if self._wait_for_download(before_files, file_name, target_info_with_sub):
                                        downloaded_files.append("downloaded")
                                    
                                    # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                                    self._mark_as_processed(unique_key, file_name, downloaded_files)
                                    results.append({
                                        "source": name,
                                        "category": esg_cat['code'],
                                        "sub_category": sub_label,
                                        "title": file_name,
                                        "files": downloaded_files
                                    })
                                    
                                except Exception as dl_err:
                                    print(f"         âš ï¸ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {dl_err}")
                            
                            # ì²´í¬ë°•ìŠ¤ í•´ì œ
                            if sub_checkbox.is_selected():
                                driver.execute_script("arguments[0].click();", sub_checkbox)
                                time.sleep(0.5)
                            
                            print(f"      âœ“ [{idx+1}] {sub_label} ì²˜ë¦¬ ì™„ë£Œ")
                                
                        except Exception as sub_err:
                            print(f"      âš ï¸ í•˜ìœ„ í•­ëª© ì˜¤ë¥˜: {sub_err}")
                            continue
                    
                    print(f"   âœ… [{esg_cat['code']}] ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì™„ë£Œ!\n")
                        
                except Exception as cat_err:
                    print(f"   âŒ {esg_cat['code']} ì¹´í…Œê³ ë¦¬ ì˜¤ë¥˜: {cat_err}")
                    import traceback
                    traceback.print_exc()
                    continue
                    
        except Exception as e:
            print(f"âŒ ESG Hub í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
        return results

    # ... (KOSHA, Google Fallback ë“± ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€) ...
    def _scrape_kosha_archive(self, driver, target_info: Dict) -> List[Dict]:
        # (ê¸°ì¡´ KOSHA í¬ë¡¤ëŸ¬ ë¡œì§ ìœ ì§€)
        url = target_info["url"]
        name = target_info["name"]
        results = []
        print(f"ğŸ“¡ [{name}] KOSHA ì ‘ì† ì¤‘... ({url})")
        try:
            driver.get(url)
            wait = WebDriverWait(driver, 20)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
            time.sleep(3) 
            rows = driver.find_elements(By.CSS_SELECTOR, "table tbody tr")
            for i in range(min(5, len(rows))):
                try:
                    rows = driver.find_elements(By.CSS_SELECTOR, "table tbody tr")
                    if i >= len(rows): break
                    row = rows[i]
                    cols = row.find_elements(By.TAG_NAME, "td")
                    if len(cols) < 5: continue
                    title = cols[2].text.strip()
                    unique_key = f"{name}_{title}"
                    if self._is_processed(unique_key):
                        print(f"   â­ï¸ [Skip] {title}")
                        continue
                    print(f"   ğŸ” [New] ë¶„ì„: {title}")
                    file_col = cols[4]
                    target_btn = None
                    try: target_btn = file_col.find_element(By.CSS_SELECTOR, "a.download")
                    except:
                        try: target_btn = file_col.find_element(By.CSS_SELECTOR, "a[class*='down']")
                        except:
                            try:
                                img = file_col.find_element(By.TAG_NAME, "img")
                                target_btn = img.find_element(By.XPATH, "./..")
                            except: pass
                    if target_btn:
                        before_files = set(os.listdir(DOWNLOAD_DIR))
                        driver.execute_script("arguments[0].click();", target_btn)
                        time.sleep(3)
                        downloaded_files = []
                        if self._wait_for_download(before_files, title, target_info):
                            downloaded_files.append("downloaded")
                        self._mark_as_processed(unique_key, title, downloaded_files)
                        results.append({"source": name, "title": title, "files": downloaded_files})
                except Exception as e: print(f"      âš ï¸ Row {i} Error: {e}")
        except Exception as e: print(f"âŒ KOSHA Error: {e}")
        return results

    def _scrape_google_fallback(self, driver, target_info: Dict) -> List[Dict]:
        # (ê¸°ì¡´ Google Fallback ë¡œì§ ìœ ì§€)
        query = target_info.get("google_query")
        if not query: return []
        search_url = f"https://www.google.com/search?q={urllib.parse.quote(query)}"
        name = target_info["name"]
        results = []
        print(f"ğŸš€ [Google Bypass] '{name}' ìš°íšŒ ê²€ìƒ‰... ({query})")
        try:
            driver.get(search_url)
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "search")))
            links = driver.find_elements(By.CSS_SELECTOR, "a")
            pdf_links = []
            for link in links:
                href = link.get_attribute("href")
                if href and (href.lower().endswith(".pdf") or href.lower().endswith(".hwp")):
                    pdf_links.append((link, href))
            seen_urls = set()
            unique_files = []
            for l, h in pdf_links:
                if h not in seen_urls:
                    unique_files.append((l, h))
                    seen_urls.add(h)
            for i, (link_elem, file_url) in enumerate(unique_files[:3]):
                try:
                    title = link_elem.text or "Untitled"
                    unique_key = f"Google_{name}_{title}"
                    if self._is_processed(unique_key):
                        print(f"   â­ï¸ [Skip] {title}")
                        continue
                    print(f"   ğŸ“¥ [Direct Download] {title}")
                    response = requests.get(file_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=30)
                    if response.status_code == 200:
                        ext = os.path.splitext(file_url)[1] or ".pdf"
                        safe_title = "".join([c for c in title if c.isalnum() or c in (' ', '-', '_', '.')]).rstrip()[:50]
                        filename = f"{safe_title}{ext}"
                        file_path = os.path.join(DOWNLOAD_DIR, filename)
                        with open(file_path, 'wb') as f: f.write(response.content)
                        print(f"      âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename}")
                        if self._analyze_and_store(file_path, title, target_info):
                            self._mark_as_processed(unique_key, title, [file_path])
                            results.append({"source": name, "title": title, "files": [file_path]})
                except Exception as e: print(f"      âš ï¸ File Error: {e}")
        except Exception as e: print(f"âŒ Google Error: {e}")
        return results

    def collect_all_guides(self) -> str:
        print("\n" + "="*50)
        print(f"ğŸ›¡ï¸ [Risk Data ìˆ˜ì§‘] {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*50)
        
        driver = self._get_chrome_driver()
        total_results = []
        
        try:
            for target in RISK_TARGETS:
                if target.get("type") == "KOSHA_ARCHIVE":
                    res = self._scrape_kosha_archive(driver, target)
                elif target.get("type") == "ESG_HUB":
                    res = self._scrape_esg_finance_hub(driver, target)
                else:
                    res = self._scrape_google_fallback(driver, target)
                total_results.extend(res)
        finally:
            driver.quit()
            
        report = f"## ğŸ›¡ï¸ ë¦¬ìŠ¤í¬ ì§„ë‹¨ ìë£Œ ìˆ˜ì§‘ ë¦¬í¬íŠ¸\n"
        if total_results:
            for item in total_results:
                files = f"{len(item['files'])}ê°œ íŒŒì¼" if item['files'] else "ì—†ìŒ"
                report += f"- **[{item['source']}]** {item['title']} (ğŸ’¾ {files})\n"
        else:
            report += "- ì‹ ê·œ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            
        print(report)
        return report

_risk_collector = RiskCrawlingTool()

@tool
def fetch_risk_guides(query: str = "safety checklist") -> str:
    """
    Collects practical risk assessment guides and checklists from KOSHA, MOEL, ME, FTC, and ESG Finance Hub.
    Uses Google Search fallback for government sites.
    """
    return _risk_collector.collect_all_guides()

if __name__ == "__main__":
    _risk_collector.collect_all_guides()